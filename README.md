# Adversarial Attacks

This project demonstrates the concept of adversarial attacks on pre-trained models using Python and popular machine learning libraries.

## Overview

Adversarial attacks are techniques used to fool machine learning models by subtly modifying the input data. This project includes the following sections:

1. **Data Loading and Visualization**: This section loads and visualizes the images that will be used for the adversarial attacks.
2. **Model Loading and Prediction**: This section loads pre-trained models and uses them to make predictions on the images.
3. **Adversarial Attack**: This section implements an adversarial attack, which involves subtly modifying the images in a way that causes the models to make incorrect predictions.
4. **Ensemble Methods**: This section demonstrates how ensemble methods can be used to improve the robustness of the models against adversarial attacks.
5. **Saliency Maps**: This section generates saliency maps, which visualize the parts of the images that the models are focusing on when making their predictions.


